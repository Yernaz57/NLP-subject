{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Hypernyms.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T17:56:22.948195Z",
          "start_time": "2020-02-12T17:56:22.944176Z"
        },
        "id": "4ffy5hVvrPk6",
        "colab_type": "text"
      },
      "source": [
        "### Lab 2: Hyponyms and Hypernyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:16.470920Z",
          "start_time": "2020-02-12T14:12:15.640262Z"
        },
        "id": "AB36eG2hrPlD",
        "colab_type": "code",
        "colab": {},
        "outputId": "df89d14c-dc53-4fb2-cba2-465d913737fa"
      },
      "source": [
        "!pip install wikipedia\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual,widgets\n",
        "from IPython.display import display\n",
        "import json\n",
        "num_cores = multiprocessing.cpu_count()\n",
        "print(num_cores)\n",
        "wikipedia.set_lang(\"ru\")\n",
        "# DATA_PATH_LIST = ['D:','src2','taxonomy-enrichment','data','training_data']\n",
        "DATA_PATH_LIST = ['.']\n",
        "EMBEDDING_MODEL_FILENAME = \"wiki_node2vec.bin\"\n",
        "DATA_PATH=\"/\".join(DATA_PATH_LIST+[\"training_nouns.tsv\"])\n",
        "df = pd.read_csv(DATA_PATH,sep='\\t')\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wikipedia in c:\\users\\ernaz\\anaconda3\\lib\\site-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\ernaz\\anaconda3\\lib\\site-packages (from wikipedia) (4.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\ernaz\\anaconda3\\lib\\site-packages (from wikipedia) (2.22.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\ernaz\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (1.9.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ernaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ernaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ernaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ernaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.9.11)\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:16.475780Z",
          "start_time": "2020-02-12T14:12:16.473114Z"
        },
        "id": "qiOE-BljrPlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prestr(x):\n",
        "    return str(x).replace('\\\"','').replace(\"'\",'\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:16.685506Z",
          "start_time": "2020-02-12T14:12:16.477634Z"
        },
        "id": "rFdHGtjfrPls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DefDict(defaultdict):\n",
        "    def __missing__(self, key):\n",
        "        self[key] = key\n",
        "        return key\n",
        "    \n",
        "idx2syns = DefDict(lambda x:x)\n",
        "for val in df.values:\n",
        "    idx2syns[val[0]]=val[1]\n",
        "    try:\n",
        "        pidxs = json.loads(prestr(val[2]))\n",
        "        concp = [el.split(\",\")[0] for el in json.loads(prestr(val[3]))]\n",
        "        idx2syns.update(dict(zip(pidxs,concp)))\n",
        "    except:\n",
        "        print(prestr(val[2]))\n",
        "        print(prestr(val[3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T17:12:34.060220Z",
          "start_time": "2020-02-12T17:12:34.055235Z"
        },
        "id": "KogiIrklrPl7",
        "colab_type": "text"
      },
      "source": [
        "### Interactive visualization of hyponyms and hypernyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T17:11:37.966489Z",
          "start_time": "2020-02-12T17:11:37.931688Z"
        },
        "id": "p4U71IvbrPmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pygraphviz\n",
        "button = widgets.Button(description=\"Draw\")\n",
        "query = widgets.Text(\n",
        "    value='МАТЬ',\n",
        "    placeholder='Query',\n",
        "    description='String:',\n",
        "    disabled=False\n",
        ")\n",
        "display(button,query)\n",
        "\n",
        "def creategraph(df):\n",
        "    res = []\n",
        "    for row in df.values:\n",
        "        cohyps = row[1].split(\",\")\n",
        "        for idx,cohyp in enumerate(cohyps):\n",
        "            for parent in json.loads(prestr(row[2])):\n",
        "                res.append((row[0]+'-'+str(idx),parent))\n",
        "    return res\n",
        "\n",
        "def graphdraw(b):\n",
        "    print(\"graphdraw\",query.value)\n",
        "    subset = df[df['TEXT'].str.contains(query.value.upper())]\n",
        "    g = nx.DiGraph()\n",
        "    for el in subset.values:\n",
        "        cohyps = el[1].split(\",\")\n",
        "        print(cohyps)\n",
        "        syns = idx2syns[el[0]]\n",
        "        for child in cohyps:\n",
        "            for parent in json.loads(prestr(el[2])):\n",
        "                ed = g.add_edge(child,idx2syns[parent],label=\"is a\")\n",
        "            \n",
        "    plt.figure(figsize=(15,15))\n",
        "    pos = nx.nx_agraph.graphviz_layout(g)\n",
        "    nx.draw(g,with_labels=True,pos=pos)\n",
        "#     edge_labels=nx.draw_networkx_edge_labels(g,pos=pos)\n",
        "    plt.show()\n",
        "button.on_click(graphdraw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T11:38:20.619158Z",
          "start_time": "2020-02-12T11:38:20.614734Z"
        },
        "id": "07hOu8KTrPmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Pattern extractor\n",
        "\n",
        "Yargy — библиотека для извлечения структурированной информации из текстов на русском языке. Правила описываются контекстно-свободными грамматиками и словарями ключевых слов. Банк готовых правил для имён, дат, адресов и других сущностей доступен в репозитории Natasha.\n",
        "* https://yargy.readthedocs.io/ru/latest/\n",
        "* http://pymorphy2.readthedocs.io/en/latest/user/grammemes.html\n",
        "* https://github.com/natasha/natasha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-02-12T13:09:50.486Z"
        },
        "id": "XJz-0bw6rPmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Токенизатор"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:16.819980Z",
          "start_time": "2020-02-12T14:12:16.708109Z"
        },
        "id": "0PGD2kXjrPmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "db64ea86-aa91-4f88-c61d-56bc818c4948"
      },
      "source": [
        "!pip install yargy\n",
        "\n",
        "from yargy.tokenizer import MorphTokenizer\n",
        "\n",
        "\n",
        "tokenizer = MorphTokenizer()\n",
        "text = '''Ростов-на-Дону\n",
        "Длительностью 18ч. 10мин.\n",
        "Яндекс.Такси\n",
        "π ≈ 3.1415\n",
        "1 500 000$\n",
        "http://vk.com\n",
        "'''\n",
        "for line in text.splitlines():\n",
        "    print([_.value for _ in tokenizer(line)])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yargy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/64/d6abf637228bed6b0249b522f588d19dca9f09ab65db13bef41096f51889/yargy-0.12.0-py2.py3-none-any.whl (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 40kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
            "\u001b[?25hCollecting backports.functools-lru-cache==1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/40/0b1db94fdfd71353ae67ec444ff28e0a7ecc25212d1cb94c291b6cd226f9/backports.functools_lru_cache-1.3-py2.py3-none-any.whl\n",
            "Collecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->yargy) (0.6.2)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 41.0MB/s \n",
            "\u001b[?25hInstalling collected packages: backports.functools-lru-cache, dawg-python, pymorphy2-dicts, pymorphy2, yargy\n",
            "Successfully installed backports.functools-lru-cache-1.3 dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 yargy-0.12.0\n",
            "['Ростов', '-', 'на', '-', 'Дону']\n",
            "['Длительностью', '18', 'ч', '.', '10', 'мин', '.']\n",
            "['Яндекс', '.', 'Такси']\n",
            "['π', '≈', '3', '.', '1415']\n",
            "['1', '500', '000', '$']\n",
            "['http', ':', '/', '/', 'vk', '.', 'com']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T13:11:56.582467Z",
          "start_time": "2020-02-12T13:11:56.566175Z"
        },
        "id": "IAJM7g_frPm5",
        "colab_type": "text"
      },
      "source": [
        "# Газеттир\n",
        "Газеттир нужен для удобной работы с последовательностью слов. Например, можно написать:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:16.844412Z",
          "start_time": "2020-02-12T14:12:16.821582Z"
        },
        "id": "WPMXZbZYrPm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yargy import or_, rule\n",
        "from yargy.predicates import normalized\n",
        "\n",
        "RULE = or_(\n",
        "    rule(normalized('dvd'), '-', normalized('диск')),\n",
        "    rule(normalized('видео'), normalized('файл'))\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:16.968622Z",
          "start_time": "2020-02-12T14:12:16.846737Z"
        },
        "id": "1m0OxsTwrPnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yargy import Parser\n",
        "from yargy.pipelines import morph_pipeline\n",
        "\n",
        "\n",
        "RULE = morph_pipeline([\n",
        "    'dvd-диск',\n",
        "    'видео файл',\n",
        "    'видеофильм',\n",
        "    'газета',\n",
        "    'электронный дневник',\n",
        "    'эссе',\n",
        "])\n",
        "\n",
        "parser = Parser(RULE)\n",
        "text = 'Видео файл на dvd-диске'\n",
        "for match in parser.findall(text):\n",
        "    print([_.value for _ in match.tokens])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:54.576104Z",
          "start_time": "2020-02-12T14:12:54.511149Z"
        },
        "id": "iWHZ3jgGrPnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yargy import Parser, rule, and_\n",
        "from yargy.predicates import gram, is_capitalized, dictionary\n",
        "\n",
        "\n",
        "GEO = rule(\n",
        "    and_(\n",
        "        gram('ADJF'),  # так помечается прилагательное, остальные пометки описаны в\n",
        "                       # http://pymorphy2.readthedocs.io/en/latest/user/grammemes.html\n",
        "        is_capitalized()\n",
        "    ),\n",
        "    gram('ADJF').optional().repeatable(),\n",
        "    dictionary({\n",
        "        'федерация',\n",
        "        'республика'\n",
        "    })\n",
        ")\n",
        "\n",
        "\n",
        "parser = Parser(GEO)\n",
        "text = '''\n",
        "В Чеченской республике на день рождения ...\n",
        "Донецкая народная республика провозгласила ...\n",
        "Башня Федерация — одна из самых высоких ...\n",
        "'''\n",
        "for match in parser.findall(text):\n",
        "    print([_.value for _ in match.tokens])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T13:13:57.167613Z",
          "start_time": "2020-02-12T13:13:57.159920Z"
        },
        "id": "AZfBbOjtrPna",
        "colab_type": "text"
      },
      "source": [
        "### Предикаты\n",
        "\n",
        "Предикат — функция, которая принимает на вход токен и возвращает True или False. В Yargy встроено много готовых предикатов. Полный список есть в справочнике. Предикаты комбинируются с помощью and_, or_ и not_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:17.173350Z",
          "start_time": "2020-02-12T14:12:17.136108Z"
        },
        "id": "Wghp0FAcrPnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yargy import and_, not_\n",
        "from yargy.tokenizer import MorphTokenizer\n",
        "from yargy.predicates import is_capitalized, eq\n",
        "\n",
        "\n",
        "tokenizer = MorphTokenizer()\n",
        "token = next(tokenizer('Стали'))\n",
        "\n",
        "predicate = is_capitalized()\n",
        "print(predicate(token))\n",
        "\n",
        "predicate = and_(\n",
        "    is_capitalized(),\n",
        "    not_(eq('марки'))\n",
        ")\n",
        "print(predicate(token))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T13:15:56.600763Z",
          "start_time": "2020-02-12T13:15:56.596609Z"
        },
        "id": "1giGxEG2rPno",
        "colab_type": "text"
      },
      "source": [
        "### Грамматики\n",
        "В Yargy используется специальный DSL для описания грамматик. Любую контекстно-свободную грамматику можно описать с помощью конструкций Питона. Например, есть примитивная грамматика для размеров одежды:\n",
        "\n",
        "KEY -> р. | размер\n",
        "\n",
        "VALUE -> S | M | L\n",
        "\n",
        "SIZE -> KEY VALUE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:14:21.602988Z",
          "start_time": "2020-02-12T14:14:21.589310Z"
        },
        "id": "KHmt9EeHrPns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yargy import rule, or_\n",
        "\n",
        "\n",
        "KEY = or_(\n",
        "    rule('р', '.'),\n",
        "    rule('размер')\n",
        ").named('KEY')\n",
        "VALUE = or_(\n",
        "    rule('S'),\n",
        "    rule('M'),\n",
        "    rule('L'),\n",
        "    rule('XS'),\n",
        ").named('VALUE')\n",
        "SIZE = rule(\n",
        "    KEY,\n",
        "    VALUE\n",
        ").named('SIZE')\n",
        "SIZE.normalized.as_bnf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:14:27.723857Z",
          "start_time": "2020-02-12T14:14:27.662113Z"
        },
        "id": "1XzgP0TLrPnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parser = Parser(\n",
        "    SIZE\n",
        ")\n",
        "text = 'размер M; размер A; размер XS;'\n",
        "for match in parser.findall(text):\n",
        "    print([_.value for _ in match.tokens])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:19.308726Z",
          "start_time": "2020-02-12T14:12:17.354354Z"
        },
        "id": "6JGihHelrPn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yargy import Parser, rule, and_, or_, not_\n",
        "from yargy.interpretation import fact, attribute\n",
        "from yargy.predicates import gram, is_capitalized, dictionary, eq\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook\n",
        "from gensim import utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:19.323139Z",
          "start_time": "2020-02-12T14:12:19.310769Z"
        },
        "id": "l-xRexdgrPoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "START = rule(\n",
        "    or_(\n",
        "        rule(gram('ADJF')),\n",
        "        rule(gram('NOUN'))\n",
        "    ).optional(),\n",
        "    gram('NOUN')\n",
        ")\n",
        "\n",
        "START_S = or_(\n",
        "    eq('такой'),\n",
        "    eq('такие'),\n",
        ")\n",
        "\n",
        "KAK = eq('как')\n",
        "INCLUDING = or_(\n",
        "    or_(\n",
        "        eq('в'),\n",
        "        eq('том'),\n",
        "        eq('числе'),\n",
        "    ),\n",
        "    eq('включающий'),\n",
        "    or_(\n",
        "        eq('включающий'),\n",
        "        eq('в'),\n",
        "        eq('себя'),\n",
        "    ),\n",
        "    or_(\n",
        "        eq('включающие'),\n",
        "        eq('в'),\n",
        "        eq('себя'),\n",
        "    ),\n",
        "    eq('включающие'),\n",
        "    eq('особенно'),\n",
        "\n",
        ")\n",
        "\n",
        "MID_S = or_(\n",
        "    rule(\n",
        "        or_(\n",
        "            eq('такой'),\n",
        "            eq('такие'),\n",
        "        ),\n",
        "        eq('как')\n",
        "    )\n",
        ")\n",
        "ATAKJE = rule(\n",
        "    eq(','),\n",
        "    eq('а'),\n",
        "    eq('также')\n",
        ")\n",
        "\n",
        "MID = or_(\n",
        "    rule(\n",
        "        eq('это')\n",
        "    ),\n",
        "    rule(\n",
        "        eq('—')\n",
        "    ),\n",
        "    rule(\n",
        "        eq('—'),\n",
        "        eq('это')\n",
        "    ),\n",
        "    rule(\n",
        "        eq('—'),\n",
        "        not_(eq('км'))\n",
        "    ),\n",
        "    rule(\n",
        "        or_(\n",
        "            eq('и'),\n",
        "            eq('или'),\n",
        "        ),\n",
        "        eq('другие')\n",
        "    )\n",
        ")\n",
        "\n",
        "END = or_(\n",
        "    rule(\n",
        "        gram('NOUN'),\n",
        "        gram('NOUN')\n",
        "    ),\n",
        "    rule(\n",
        "        gram('ADJF').repeatable(),\n",
        "        gram('NOUN')\n",
        "    ),\n",
        "    rule(\n",
        "        gram('ADJF'),\n",
        "        gram('ADJF').repeatable(),\n",
        "        gram('NOUN')\n",
        "    ),\n",
        "    rule(\n",
        "        gram('NOUN').repeatable(),\n",
        "        gram('ADJF'),\n",
        "        gram('NOUN').repeatable()\n",
        "    ),\n",
        "    rule(\n",
        "        gram('NOUN').repeatable()\n",
        "    )\n",
        ")\n",
        "\n",
        "Item = fact(\n",
        "    'Item',\n",
        "    [attribute('titles').repeatable()]\n",
        ")\n",
        "\n",
        "\n",
        "IGNORE = rule(\n",
        "    '(',\n",
        "    not_(eq(')')).repeatable(),\n",
        "    ')'\n",
        ")\n",
        "\n",
        "ITEM = rule(\n",
        "    IGNORE.interpretation(\n",
        "        Item.titles\n",
        "    ),\n",
        "    eq(',').optional() \n",
        ").repeatable().interpretation(\n",
        "    Item\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:19.434702Z",
          "start_time": "2020-02-12T14:12:19.324707Z"
        },
        "id": "SesNB-n1rPoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_hyperonyms(main_word):\n",
        "    HYPONYM = eq(utils.deaccent(main_word))\n",
        "    RULE = or_(\n",
        "        rule(HYPONYM, ATAKJE, START, MID, END),\n",
        "        rule(HYPONYM, MID, END),\n",
        "        rule(START_S, END, KAK, HYPONYM),\n",
        "        rule(END, INCLUDING, HYPONYM)\n",
        "    )\n",
        "    parser = Parser(RULE) \n",
        "    text = utils.deaccent(wikipedia.summary(main_word))\n",
        "    print(text)\n",
        "    text = re.sub(r'\\(.+?\\)', '', text)\n",
        "    text = text.lower().replace('* сергии радонежскии* ', '')\n",
        "    for idx, match in enumerate(parser.findall(text.lower())):\n",
        "        k = [_.value for _ in match.tokens]\n",
        "        print(k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-12T14:12:22.648500Z",
          "start_time": "2020-02-12T14:12:19.437840Z"
        },
        "id": "nChBzAd6rPoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_hyperonyms(\"банан\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOH6U1aIrPoe",
        "colab_type": "text"
      },
      "source": [
        "#### Task 1 (deadline 19.02.2020 24:00)\n",
        "* Find your name on the spreadsheet https://docs.google.com/spreadsheets/d/1RR2I6toCkebbGU1UK83HS70Ru_l0_o-nnZIHyiFB0No/edit?usp=sharing. In opposite of your name there are 24 words of hyponyms, you have to insert five corresponding hypernyms next to them. Examples of hyponyms and hyperonyms relationship you can find above in the current Jupiter notebook.\n",
        "* Find for each pair of hyponyms and hypernyms a corresponding snippet of a text with their mentions. The source of the text can be any free resources, e.g., Wikipedia, Google, Yandex, others. You should save the snippets and their URLs within the lab2 folder in your NLP git-repo with .csv file-extension in a single file.\n",
        "\n",
        "#### Task 2 (deadline 26.02.2020 24:00)\n",
        "* It would be best if you created a pandas DataFrame of the texts from the previous task. And apply to the DataFrame the function 'get_hyperonyms,' which must return the list of the corresponding hypernyms from the text automatically. If there are errors or misses, you should fix them in the code for your case of the 24 words. Nevertheless, it is strictly prohibited to use hard coding. Save your notebook with parser code within the lab2 folder in your NLP git-repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifIkUb8QrPog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "111a8d3e-eb97-4a00-d11a-d08c54780695"
      },
      "source": [
        "import pandas as pd\n",
        "homework_data = pd.read_excel('Yernaz_Words.xlsx', sheet_name='Лист1')\n",
        "\n",
        "homework_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Number of words</th>\n",
              "      <th>Number of student</th>\n",
              "      <th>Name</th>\n",
              "      <th>HYPONYM</th>\n",
              "      <th>HYPERONYM</th>\n",
              "      <th>HYPERONYM.1</th>\n",
              "      <th>HYPERONYM.2</th>\n",
              "      <th>HYPERONYM.3</th>\n",
              "      <th>HYPERONYM.4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>433</td>\n",
              "      <td>19</td>\n",
              "      <td>Bizhanov Yernaz</td>\n",
              "      <td>ИЗЫСКАТЕЛЬ</td>\n",
              "      <td>Исследователь</td>\n",
              "      <td>Изучение</td>\n",
              "      <td>Анализ</td>\n",
              "      <td>Работы</td>\n",
              "      <td>Действия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>434</td>\n",
              "      <td>19</td>\n",
              "      <td>Bizhanov Yernaz</td>\n",
              "      <td>ИЗЮБР</td>\n",
              "      <td>Олень</td>\n",
              "      <td>Рогач</td>\n",
              "      <td>Парнокопытный</td>\n",
              "      <td>Травоядный</td>\n",
              "      <td>Животное</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>435</td>\n",
              "      <td>19</td>\n",
              "      <td>Bizhanov Yernaz</td>\n",
              "      <td>ИКОНКА</td>\n",
              "      <td>Рисунок</td>\n",
              "      <td>Изображение</td>\n",
              "      <td>Логотип</td>\n",
              "      <td>Символика</td>\n",
              "      <td>Картина</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>436</td>\n",
              "      <td>19</td>\n",
              "      <td>Bizhanov Yernaz</td>\n",
              "      <td>ИКОНОБОРЧЕСТВО</td>\n",
              "      <td>Движение</td>\n",
              "      <td>Течение</td>\n",
              "      <td>Разрушители</td>\n",
              "      <td>Отвержение</td>\n",
              "      <td>Оппозиция</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>437</td>\n",
              "      <td>19</td>\n",
              "      <td>Bizhanov Yernaz</td>\n",
              "      <td>ИНДУКТОР</td>\n",
              "      <td>Машина</td>\n",
              "      <td>Изобретение</td>\n",
              "      <td>Прибор</td>\n",
              "      <td>Тело</td>\n",
              "      <td>Устройство</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Number of words  ...  HYPERONYM.3 HYPERONYM.4\n",
              "0         NaN              433  ...       Работы    Действия\n",
              "1         NaN              434  ...   Травоядный    Животное\n",
              "2         NaN              435  ...    Символика     Картина\n",
              "3         NaN              436  ...   Отвержение   Оппозиция\n",
              "4         NaN              437  ...         Тело  Устройство\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx2T61JIrwMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmHQOcTkrPop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "4e3d7d04-ce93-41ce-d5e5-5e626426957f"
      },
      "source": [
        "    print('Гиперонимы для слова: ', 'ИЗЫСКАТЕЛЬ')\n",
        "    print('\\n')\n",
        "    get_hyperonyms('ИЗЫСКАТЕЛЬ')\n",
        "    print('\\n\\n')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Гиперонимы для слова:  ИЗЫСКАТЕЛЬ\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-596df2cbb83f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Гиперонимы для слова: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ИЗЫСКАТЕЛЬ'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_hyperonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ИЗЫСКАТЕЛЬ'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_hyperonyms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC9Wm_jXrPoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}